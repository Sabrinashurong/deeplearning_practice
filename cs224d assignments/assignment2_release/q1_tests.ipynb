{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  \"\"\"\n",
    "  Compute the softmax function in tensorflow.\n",
    "\n",
    "  You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "  tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "  not need to use all of these functions). Recall also that many common\n",
    "  tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "  if x and y are both tensors). Make sure to implement the numerical stability\n",
    "  fixes as in the previous homework!\n",
    "\n",
    "  Args:\n",
    "    x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "         represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "         input as in the previous homework)\n",
    "  Returns:\n",
    "    out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "         tensor in this problem.\n",
    "  \"\"\"\n",
    "\n",
    "  ### YOUR CODE HERE\n",
    "\n",
    "  log_c = tf.reduce_max(x, reduction_indices=[len(x.get_shape()) - 1], keep_dims=True)\n",
    "  print(\"log_c: \",log_c)\n",
    "  y     = tf.reduce_sum(tf.exp(x - log_c), axis=[len(x.get_shape()) - 1], keep_dims=True)\n",
    "  print(\"y: \",y.eval())\n",
    "  out   = tf.exp(x - log_c) / y\n",
    "  return out\n",
    "\n",
    "  ### END YOUR CODE\n",
    "  #out = tf.nn.softmax(x)\n",
    "  #return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "  \"\"\"\n",
    "  Some simple tests to get you started. \n",
    "  Warning: these are not exhaustive.\n",
    "  \"\"\"\n",
    "  print(\"Running basic tests...\")\n",
    "  with tf.Session() as sess:\n",
    "      test1 = softmax(tf.convert_to_tensor(\n",
    "          np.array([[1001,1002],[3,4]]), dtype=tf.float32))\n",
    "      test1 = test1.eval()\n",
    "      assert np.amax(np.fabs(test1 - np.array(\n",
    "          [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "      test2 = softmax(tf.convert_to_tensor(\n",
    "          np.array([[-1001,-1002]]), dtype=tf.float32))\n",
    "      with tf.Session():\n",
    "          test2 = test2.eval()\n",
    "      assert np.amax(np.fabs(test2 - np.array(\n",
    "          [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "      print(\"Basic (non-exhaustive) softmax tests pass\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "log_c:  Tensor(\"Max_11:0\", shape=(2, 1), dtype=float32)\n",
      "y:  [[ 1.36787939]\n",
      " [ 1.36787939]]\n",
      "log_c:  Tensor(\"Max_12:0\", shape=(1, 1), dtype=float32)\n",
      "y:  [[ 1.36787939]]\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, yhat):\n",
    "  \"\"\"\n",
    "  Compute the cross entropy loss in tensorflow.\n",
    "\n",
    "  y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "  of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "  be of dtype tf.float32.\n",
    "\n",
    "  The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "  solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "  Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "        functions.\n",
    "\n",
    "  Args:\n",
    "    y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "    yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "          probability distribution and should sum to 1.\n",
    "  Returns:\n",
    "    out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "          tensor in the problem.\n",
    "  \"\"\"\n",
    "  ### YOUR CODE HERE\n",
    "  y = tf.to_float(y)\n",
    "  out = -tf.reduce_sum(tf.multiply(y,tf.log(yhat)))\n",
    "  #raise NotImplementedError\n",
    "  ### END YOUR CODE\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_cross_entropy_loss_basic():\n",
    "  \"\"\"\n",
    "  Some simple tests to get you started.\n",
    "  Warning: these are not exhaustive.\n",
    "  \"\"\"\n",
    "  y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "  yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "  test1 = cross_entropy_loss(\n",
    "      tf.convert_to_tensor(y, dtype=tf.int32),\n",
    "      tf.convert_to_tensor(yhat, dtype=tf.float32))\n",
    "  with tf.Session():\n",
    "    test1 = test1.eval()\n",
    "  result = -3 * np.log(.5)\n",
    "  assert np.amax(np.fabs(test1 - result)) <= 1e-6\n",
    "  print(\"Basic (non-exhaustive) cross-entropy tests pass\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic (non-exhaustive) cross-entropy tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cross_entropy_loss_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
